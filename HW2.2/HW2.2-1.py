{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b444fe-1e41-4b55-a0ce-37bba0f38e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# UNIVARIABLE REGRESSION EXAMPLE\n",
    "#--------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "#------------------------\n",
    "#CODE PARAMETERS\n",
    "#------------------------\n",
    "\n",
    "#USER PARAMETERS\n",
    "IPLOT=True\n",
    "\n",
    "\n",
    "PARADIGM='batch'\n",
    "\n",
    "\n",
    "model_type=\"linear\";X_KEYS=['x1','x2']; Y_KEYS=['y'];NFIT=len(X_KEYS)+1\n",
    "\n",
    "file='planar_x1_x2_y.json'\n",
    "with open(file) as json_file:\n",
    "\tinput1=json.load(json_file)\n",
    "\n",
    "#SAVE HISTORY FOR PLOTTING AT THE END\n",
    "epoch=1; epochs=[]; loss_train=[];  loss_val=[]\n",
    "\n",
    "\n",
    "#------------------------\n",
    "#CONVERT TO MATRICES AND NORMALIZE\n",
    "#------------------------\n",
    "\n",
    "#CONVERT DICTIONARY INPUT AND OUTPUT MATRICES #SIMILAR TO PANDAS DF   \n",
    "X=[]; Y=[]\n",
    "for key in input1.keys():\n",
    "\tif(key in X_KEYS): X.append(input1[key])\n",
    "\tif(key in Y_KEYS): Y.append(input1[key])\n",
    "\n",
    "\n",
    "\n",
    "#MAKE ROWS=SAMPLE DIMENSION (TRANSPOSE)\n",
    "X=np.transpose(np.array(X))\n",
    "Y=np.transpose(np.array(Y))\n",
    "print('--------INPUT INFO-----------')\n",
    "print(\"X shape:\",X.shape); print(\"Y shape:\",Y.shape,'\\n')\n",
    "\n",
    "#TAKE MEAN AND STD DOWN COLUMNS (I.E DOWN SAMPLE DIMENSION)\n",
    "XMEAN=np.mean(X,axis=0); XSTD=np.std(X,axis=0) \n",
    "YMEAN=np.mean(Y,axis=0); YSTD=np.std(Y,axis=0) \n",
    "\n",
    "# #NORMALIZE \n",
    "# X=(X-XMEAN)/XSTD;  Y=(Y-YMEAN)/YSTD  \n",
    "\n",
    "#------------------------\n",
    "#PARTITION DATA\n",
    "#------------------------\n",
    "#TRAINING: \t DATA THE OPTIMIZER \"SEES\"\n",
    "#VALIDATION: NOT TRAINED ON BUT MONITORED DURING TRAINING\n",
    "#TEST:\t\t NOT MONITORED DURING TRAINING (ONLY USED AT VERY END)\n",
    "\n",
    "f_train=0.8; f_val=0.15; f_test=0.05;\n",
    "\n",
    "if(f_train+f_val+f_test != 1.0):\n",
    "\traise ValueError(\"f_train+f_val+f_test MUST EQUAL 1\");\n",
    "\n",
    "#PARTITION DATA\n",
    "rand_indices = np.random.permutation(X.shape[0])\n",
    "CUT1=int(f_train*X.shape[0]); \n",
    "CUT2=int((f_train+f_val)*X.shape[0]); \n",
    "train_idx, val_idx, test_idx = rand_indices[:CUT1], rand_indices[CUT1:CUT2], rand_indices[CUT2:]\n",
    "print('------PARTITION INFO---------')\n",
    "print(\"train_idx shape:\",train_idx.shape)\n",
    "print(\"val_idx shape:\"  ,val_idx.shape)\n",
    "print(\"test_idx shape:\" ,test_idx.shape)\n",
    "\n",
    "#------------------------\n",
    "#MODEL\n",
    "#------------------------\n",
    "def S(x): return 1.0/(1.0+np.exp(-x))\n",
    "def model(x,p):\n",
    "\tlinear=p[0]+np.matmul(x,p[1:].reshape(NFIT-1,1))\n",
    "\tif(model_type==\"linear\"):   return  linear\n",
    "\tif(model_type==\"logistic\"): return  S(linear)\n",
    "\n",
    "#FUNCTION TO MAKE VARIOUS PREDICTIONS FOR GIVEN PARAMETERIZATION\n",
    "def predict(p):\n",
    "\tglobal YPRED_T,YPRED_V,YPRED_TEST,MSE_T,MSE_V\n",
    "\tYPRED_T=model(X[train_idx],p)\n",
    "\tYPRED_V=model(X[val_idx],p)\n",
    "\tYPRED_TEST=model(X[test_idx],p)\n",
    "\tMSE_T=np.mean((YPRED_T-Y[train_idx])**2.0)\n",
    "\tMSE_V=np.mean((YPRED_V-Y[val_idx])**2.0)\n",
    "\n",
    "#------------------------\n",
    "#LOSS FUNCTION\n",
    "#------------------------\n",
    "def loss(p,index_2_use):\n",
    "\terrors=model(X[index_2_use],p)-Y[index_2_use]  #VECTOR OF ERRORS\n",
    "\ttraining_loss=np.mean(errors**2.0)\t\t\t\t#MSE\n",
    "\treturn training_loss\n",
    "\n",
    "#------------------------\n",
    "#MINIMIZER FUNCTION\n",
    "#------------------------\n",
    "def minimizer(f,xi, algo='GD', LR=0.01):\n",
    "\tglobal epoch,epochs, loss_train,loss_val \n",
    "\t# x0=initial guess, (required to set NDIM)\n",
    "\t# algo=GD or MOM\n",
    "\t# LR=learning rate for gradient decent\n",
    "\n",
    "\t#PARAM\n",
    "\titeration=1\t\t\t#ITERATION COUNTER\n",
    "\tdx=0.0001\t\t\t#STEP SIZE FOR FINITE DIFFERENCE\n",
    "\tmax_iter=5000\t\t#MAX NUMBER OF ITERATION\n",
    "\ttol=10**-30\t\t\t#EXIT AFTER CHANGE IN F IS LESS THAN THIS \n",
    "\tNDIM=len(xi)\t\t#DIMENSION OF OPTIIZATION PROBLEM\n",
    "\n",
    "\t#OPTIMIZATION LOOP\n",
    "\twhile(iteration<=max_iter):\n",
    "\n",
    "\t\t#-------------------------\n",
    "\t\t#DATASET PARITION BASED ON TRAINING PARADIGM\n",
    "\t\t#-------------------------\n",
    "\t\tif(PARADIGM=='batch'):\n",
    "\t\t\tif(iteration==1): index_2_use=train_idx\n",
    "\t\t\tif(iteration>1):  epoch+=1\n",
    "\t\telse:\n",
    "\t\t\tprint(\"REQUESTED PARADIGM NOT CODED\"); exit()\n",
    "\n",
    "\t\t#-------------------------\n",
    "\t\t#NUMERICALLY COMPUTE GRADIENT \n",
    "\t\t#-------------------------\n",
    "\t\tdf_dx=np.zeros(NDIM);\t#INITIALIZE GRADIENT VECTOR\n",
    "\t\tfor i in range(0,NDIM):\t#LOOP OVER DIMENSIONS\n",
    "\n",
    "\t\t\tdX=np.zeros(NDIM);  #INITIALIZE STEP ARRAY\n",
    "\t\t\tdX[i]=dx; \t\t\t#TAKE SET ALONG ith DIMENSION\n",
    "\t\t\txm1=xi-dX; \t\t\t#STEP BACK\n",
    "\t\t\txp1=xi+dX; \t\t\t#STEP FORWARD \n",
    "\n",
    "\t\t\t#CENTRAL FINITE DIFF\n",
    "\t\t\tgrad_i=(f(xp1,index_2_use)-f(xm1,index_2_use))/dx/2\n",
    "\n",
    "\t\t\t# UPDATE GRADIENT VECTOR \n",
    "\t\t\tdf_dx[i]=grad_i \n",
    "\t\t\t\n",
    "\t\t#TAKE A OPTIMIZER STEP\n",
    "\t\tif(algo==\"GD\"):  xip1=xi-LR*df_dx \n",
    "\t\tif(algo==\"MOM\"): print(\"REQUESTED ALGORITHM NOT CODED\"); exit()\n",
    "\n",
    "\t\t#REPORT AND SAVE DATA FOR PLOTTING\n",
    "\t\tif(iteration%1==0):\n",
    "\t\t\tpredict(xi)\t#MAKE PREDICTION FOR CURRENT PARAMETERIZATION\n",
    "\t\t\tprint(iteration,\"\t\",epoch,\"\t\",MSE_T,\"\t\",MSE_V) \n",
    "\n",
    "\t\t\t#UPDATE\n",
    "\t\t\tepochs.append(epoch); \n",
    "\t\t\tloss_train.append(MSE_T);  loss_val.append(MSE_V);\n",
    "\n",
    "\t\t\t#STOPPING CRITERION (df=change in objective function)\n",
    "\t\t\tdf=np.absolute(f(xip1,index_2_use)-f(xi,index_2_use))\n",
    "\t\t\tif(df<tol):\n",
    "\t\t\t\tprint(\"STOPPING CRITERION MET (STOPPING TRAINING)\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\txi=xip1 #UPDATE FOR NEXT PASS\n",
    "\t\titeration=iteration+1\n",
    "\n",
    "\treturn xi\n",
    "\n",
    "\n",
    "#------------------------\n",
    "#FIT MODEL\n",
    "#------------------------\n",
    "\n",
    "#RANDOM INITIAL GUESS FOR FITTING PARAMETERS\n",
    "po=np.random.uniform(2,1.,size=NFIT)\n",
    "\n",
    "#TRAIN MODEL USING SCIPY MINIMIZ \n",
    "p_final=minimizer(loss,po)\t\t\n",
    "print(\"OPTIMAL PARAM:\",p_final)\n",
    "predict(p_final)\n",
    "\n",
    "#------------------------\n",
    "#GENERATE PLOTS\n",
    "#------------------------\n",
    "\n",
    "#PLOT TRAINING AND VALIDATION LOSS HISTORY\n",
    "def plot_0():\n",
    "\tfig, ax = plt.subplots()\n",
    "\tax.plot(epochs, loss_train, 'o', label='Training loss')\n",
    "\tax.plot(epochs, loss_val, 'o', label='Validation loss')\n",
    "\tplt.xlabel('epochs', fontsize=18)\n",
    "\tplt.ylabel('loss', fontsize=18)\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "#FUNCTION PLOTS\n",
    "def plot_1(i,xla='x',yla='y'):\n",
    "\tfig, ax = plt.subplots()\n",
    "\tax.plot(X[train_idx][:,i], Y[train_idx],'o', label='Training') \n",
    "\tax.plot(X[val_idx][:,i] , Y[val_idx],'x', label='Validation') \n",
    "\tax.plot(X[test_idx][:,i]  , Y[test_idx],'*', label='Test') \n",
    "\tax.plot(X[train_idx][:,i] , YPRED_T,'.', label='Model') \n",
    "\tplt.xlabel(xla, fontsize=18);\tplt.ylabel(yla, fontsize=18); \tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "#PARITY PLOT\n",
    "def plot_2(xla='y_data',yla='y_predict'):\n",
    "\tfig, ax = plt.subplots()\n",
    "\tax.plot(Y[train_idx]  , YPRED_T,'*', label='Training') \n",
    "\tax.plot(Y[val_idx]    , YPRED_V,'*', label='Validation') \n",
    "\tax.plot(Y[test_idx]    , YPRED_TEST,'*', label='Test') \n",
    "\tplt.xlabel(xla, fontsize=18);\tplt.ylabel(yla, fontsize=18); \tplt.legend()\n",
    "\tplt.show()\n",
    "\t\n",
    "if(IPLOT):\n",
    "\n",
    "\tplot_0()\n",
    "\ti=0\n",
    "\tfor key in X_KEYS:\n",
    "\t\tprint(key)\n",
    "\t\tplot_1(i,xla=key,yla=Y_KEYS[0])\n",
    "\t\ti+=1\n",
    "\n",
    "\tplot_2()\n",
    "\n",
    "\n",
    "\n",
    "# #------------------------\n",
    "# #DOUBLE CHECK PART-1 OF HW2.1\n",
    "# #------------------------\n",
    "\n",
    "# x=np.array([[3],[1],[4]])\n",
    "# y=np.array([[2,5,1]])\n",
    "\n",
    "# A=np.array([[4,5,2],[3,1,5],[6,4,3]])\n",
    "# B=np.array([[3,5],[5,2],[1,4]])\n",
    "# print(x.shape,y.shape,A.shape,B.shape)\n",
    "# print(np.matmul(x.T,x))\n",
    "# print(np.matmul(y,x))\n",
    "# print(np.matmul(x,y))\n",
    "# print(np.matmul(A,x))\n",
    "# print(np.matmul(A,B))\n",
    "# print(B.reshape(6,1))\n",
    "# print(B.reshape(1,6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
